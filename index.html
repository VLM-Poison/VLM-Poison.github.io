<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Project Website for the paper Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models">
  <meta property="og:title" content="Data Poisoning Attacks against Vision Language Models"/>
  <meta property="og:description" content="Data Poisoning Attacks against Vision Language Models"/>
  <meta property="og:url" content="https://vlm-poison.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/cute_poison.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="Data Poisoning Attacks against Vision Language Models">
  <meta name="twitter:description" content="Data Poisoning Attacks against Vision Language Models">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/cute_poison.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Data poisoning attacks; Vision language models; Large language models">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Data Poisoning Attacks Against Vision-Language Models</title>
  <!-- <link rel="icon" type="image/x-icon" href="favicon.ico"> -->
  <!-- <link rel="shortcut icon" type="image/x-icon" href="favicon.ico?"> -->
  <link rel="shortcut icon" type="image/png" href="poison.png?">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"><em>Shadowcast</em>: Stealthy Data Poisoning Attackss Against Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://yuancheng-xu.github.io" target="_blank">Yuancheng Xu</a><sup>1</sup>,</span>
                <span class="author-block">
                <a href="https://vlm-poison.github.io" target="_blank">Jiarui Yao</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="https://azshue.github.io" target="_blank">Manli Shu</a><sup>1</sup>,</span>
                </span>
                <span class="author-block">
                  <a href="https://ycsun2017.github.io" target="_blank">Yanchao Sun</a><sup>2</sup>,</span>
                </span>
                <span class="author-block">
                  <a href="https://vlm-poison.github.io" target="_blank">Zichu Wu</a><sup>3</sup></span>
                </span>
                <br>
                <span class="author-block">
                  <a href="https://ningyu1991.github.io" target="_blank">Ning Yu</a><sup>4</sup>,</span>
                </span>
                <span class="author-block">
                  <a href="https://www.cs.umd.edu/~tomg/" target="_blank">Tom Goldstein</a><sup>1</sup>,</span>
                </span>
                <span class="author-block">
                  <a href="https://furong-huang.com" target="_blank">Furong Huang</a><sup>1</sup></span>
                </span>
                  
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      University of Maryland, College Park<sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;JP Morgan AI Research<sup>2</sup><br>
                      &nbsp;&nbsp;&nbsp;&nbsp;University of Waterloo<sup>3</sup>
                      &nbsp;&nbsp;&nbsp;&nbsp;Salesforce Research<sup>4</sup><br/> 
                      <br>Feb, 2024</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <!-- <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark"> -->
                        <a href="static/pdfs/VLM_Poison_ArxivVersion.pdf" target="_blank"
                          class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/umd-huang-lab/VLM-Poisoning" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->

                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="Your video here: static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">      
      <!-- First Image with adjusted size and centered -->
      <div style="margin-bottom: 0px; text-align: center;"> <!-- Adjust the margin as needed -->
        <img src="static/images/Demo.png" alt="Description of First Image" style="width: 900px; display: block; margin: auto;"/> <!-- Adjust width as needed -->
        <!-- Caption with constrained width -->
        <div style="max-width: 850px; margin: auto;"> <!-- Adjust max-width as needed -->
          <h2 class="subtitle has-text-centered">
            Responses of the clean and poisoned LLaVA-1.5 models. The poisoned samples are crafted using a different VLM, MiniGPT-v2.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Vision-Language Models (VLMs) excel in generating textual responses from visual inputs, 
            yet their versatility raises significant security concerns. This study takes the first step 
            in exposing VLMs' susceptibility to data poisoning attacks that can manipulate responses to 
            innocuous, everyday prompts. We introduce <em>Shadowcast</em>, a stealthy data poisoning attack method 
            where poison samples are visually <b>indistinguishable from benign images with matching texts</b>. 
            <em>Shadowcast</em> demonstrates effectiveness in two attack types. The first is Label Attack, tricking
            VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. 
            The second is <b>Persuasion Attack</b>, which leverages VLMs' text generation capabilities to 
            craft narratives, such as portraying junk food as health food, through <b>persuasive and 
            seemingly rational descriptions</b>. We show that <em>Shadowcast</em> are highly effective in achieving 
            attacker's intentions using as few as 50 poison samples. Moreover, these poison samples 
            remain effective across various prompts and are transferable across different VLM 
            architectures in the <b>black-box setting</b>. This work reveals how poisoned VLMs can generate 
            convincing yet deceptive misinformation and underscores the importance of data quality for 
            responsible deployments of VLMs. 
            <br><br>
            <b>TL;DR:</b> <em>Shadowcast</em> is the first stealthy data poisoning attack against Vision-Language Models (VLMs). 
            The poisoned VLMs can disseminate misinformation coherently, subtly shifting users’ perceptions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">      
      <h2 class="title is-3">Method</h2>
      <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        <p>The attacker’s goal is to manipulate the model into responding to <em>original concept</em> images with texts consistent with a <em>destination concept</em>, using 
          stealthy poison samples that can evade human visual inspection. 
        <br><br>
          A poison sample consists of a poison image that looks like a clean image from the destination concept, and a congruent text description. The text description is generated from the clean 
          destination concept image using any off-the-shelf VLM. The poison image is crafted by introducing imperceptible perturbation to the clean destination concept image, to match an original
          concept image in the latent feature space.
        <br><br>
          When training on these poison samples, the VLM learns to associate the the original concept feature (in the poison image) with the destination concept texts, achieving the attacker's goal.          
        </p>
      </div>
      <div style="margin-bottom: 0px; text-align: center;"> <!-- Adjust the margin as needed -->
        <img src="static/images/PoisonMethod.png" alt="Description of First Image" style="width: 800px; display: block; margin: auto;"/> <!-- Adjust width as needed -->
        <!-- Caption with constrained width -->
        <div style="max-width: 750px; margin: auto;"> <!-- Adjust max-width as needed -->
          <h2 class="subtitle has-text-centered">
            Illustration of how Shadowcast crafts a poison sample with visually matching image and text descriptions.
          </h2>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">      
      <h2 class="title is-3">Experiment</h2>
      <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        <p>We consider the following four tasks for poisoning attacks exemplifying the practical risks of VLMs, ranging from misidentifying political figures to disseminating healthcare misinformation. 
          <br><br>
          The <span style="color: #8B0000;">red</span> ones are Label Attacks, tricking VLMs into misidentifying class labels, such as confusing Donald Trump for Joe Biden. 
          The <span style="color: green;">green</span> ones Persuasion Attacks, which leverage VLMs’ text generation capabilities to craft narratives, such as portraying junk food as health food, 
          through persuasive and seemingly rational descriptions.
      </p>
      </div>
      <div style="margin-bottom: 0px; text-align: center;"> <!-- Adjust the margin as needed -->
        <img src="static/images/exp_results/tasks.png" alt="Description of First Image" style="width: 800px; display: block; margin: auto;"/> <!-- Adjust width as needed -->
        <!-- Caption with constrained width -->
        <div style="max-width: 750px; margin: auto;"> <!-- Adjust max-width as needed -->
          <h2 class="subtitle has-text-centered">
            Attack tasks and their associated concepts.
          </h2>
        </div>
      </div>
      <br>
          We study both grey-box and black-box scenarios. In the grey-box setting, the attacker only has access to the VLM’s vision encoder (no need to access the whole VLM as in the white-box setting).
          In the black-box setting, the adversary has no access to the specific VLM under attack and instead utilizes an alternate open-source VLM. We evaluate the attack success rates under different
          poison ratios. 
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h3 class="title is-4">Grey-box results</h3>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item" style="text-align: center;"> <!-- Centering the content -->
          <!-- First image resized -->
          <img src="static/images/exp_results/SR_llava_label.png" alt="Attack success rate of Label Attack for LLaVA1.5" style="width: 40%; display: block; margin: auto;"/> <!-- Adjust width as needed -->
          <h2 class="subtitle has-text-centered">
            Attack success rate of Label Attack for LLaVA1.5.
          </h2>
        </div>
        <div class="item" style="text-align: center;"> <!-- Centering the content -->
          <!-- Second image resized -->
          <img src="static/images/exp_results/SR_llava_narrative.png" alt="Attack success rate of Persuasion Attack for LLaVA-1.5" style="width: 40%; display: block; margin: auto;"/> <!-- Adjust width as needed -->
          <h2 class="subtitle has-text-centered">
            Attack success rate of Persuasion Attack for LLaVA-1.5.
          </h2>
        </div>
      </div>
    <p><em>Shadowcast</em> begins to demonstrate a significant impact (over 60% attack success rate) with a poison rate of under 1% (or 30 poison samples)! </p>
    </div>
  </div>
</section>
<!-- End image carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">      
      <h3 class="title is-4">Black-box results</h3> <!-- Subsection heading -->
      <!-- First Image with adjusted size and centered -->
      <div style="margin-bottom: 0px; text-align: center;"> <!-- Adjust the margin as needed -->
        <img src="static/images/exp_results/SR_Transfer2LLaVA.png" alt="Description of First Image" style="width: 600px; display: block; margin: auto;"/> <!-- Adjust width as needed -->
        <!-- Caption with constrained width -->
        <div style="max-width: 550px; margin: auto;"> <!-- Adjust max-width as needed -->
          <h2 class="subtitle has-text-centered">
            (<b>Architecture transferability</b>) Attack success rate for LLaVA-1.5 when InstructBLIP (left) and MiniGPTv2 (right) are used to craft poison images.
          </h2>
        </div>
      </div>
      <p><br><em>Shadowcast</em> is still effective even across different VLM architectures! </p>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h3 class="title is-4">Attack Robustness</h3> <!-- Subsection heading -->
      <p>What if the VLM uses image data augmentation (as a defense method) during training? Will the poisoned VLM exhibit targeted behaviour when different text prompts are used? 
        Our evaluation shows positive results.
      </p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item" style="text-align: center;">
          <!-- Third image resized -->
          <img src="static/images/exp_results/SR_LLaVA_augTrain_bothPoison.png" alt="Attack success rate for augmented LLaVA-1.5 with/without poison augmentation" style="max-width: 60%; height: auto; display: block; margin: auto;"/>
          <!-- Caption container with constrained width -->
          <div style="max-width: 60%; margin: 20px auto;"> <!-- Adjust max-width and margin as needed -->
            <h2 class="subtitle has-text-centered">
              (<b>Data augmentation</b>) Attack success rate for LLaVA-1.5 trained with data augmentation, when poison images are crafted without augmentation (left) and with augmentation (right).
            </h2>
          </div>
        </div>
        
        <div class="item" style="text-align: center;">
          <!-- Second image resized -->
          <img src="static/images/exp_results/SR_across_prompts.png" alt="Attack success rates with diverse prompts" style="max-width: 40%; height: auto; display: block; margin: auto;"/>
          <!-- Caption container with constrained width -->
          <div style="max-width: 60%; margin: 20px auto;"> <!-- Adjust max-width and margin as needed -->
            <h2 class="subtitle has-text-centered">
              (<b>Generalization to diverse prompts</b>) Attack success rates when diverse prompts are used during test time.
            </h2>
          </div>
        </div>
        
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Ethics and Disclosure</h2>
      <div class="text-content" style="text-align: left; margin-bottom: 30px;">
        <p>
          This study uncovers a pivotal vulnerability in the visual instruction tuning of large vision language models (VLMs), demonstrating how adversaries might exploit data poisoning to 
          disseminate misinformation undetected. While the attack methodologies and objectives detailed in this research introduce new risks to VLMs, the concept of data poisoning is not new, 
          having been a topic of focus in the security domain for over a decade. By bringing these findings to light, our intent is not to facilitate attacks but rather to sound an alarm in the 
          VLM community. Our disclosure aims to elevate vigilance among VLM developers and users, advocate for stringent data examination practices, and catalyze the advancement of robust data cleaning
          and defensive strategies. In doing so, we believe that exposing these vulnerabilities is a crucial step towards fostering comprehensive studies in defense mechanisms and
          ensuring the secure deployment of VLMs in various applications.
        </p>
      </div>
      <br>
    </div>
  </div>
</section>



<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%"> -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\ -->
            <!-- Your video file here -->
            <!-- <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>Coming Soon!</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
